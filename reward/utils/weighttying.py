from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
import torch.nn as nn
import torch

# === ê²½ë¡œ ë° ì„¤ì • ===
LOCAL_MODEL_PATH = "/home/dongyoon/penaltyrm/models/PRM-qwen2.5-3b-alpacafarm-sft/checkpoint-313"
HF_HUB_REPO = "Hahmdong/PRM-qwen2.5-3b-alpacafarm-sft"  # ì—…ë¡œë“œí•  Repo ì´ë¦„

# === 1. ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ===
print("ğŸ”„ Loading original model...")
base_model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_PATH, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH, trust_remote_code=True)

# === 2. ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ì •ì˜ ===
#   Qwen2ForCausalLMì„ ìƒì†í•˜ì—¬ lm_headë¥¼ í¬í•¨ì‹œí‚¤ëŠ” ìƒˆ í´ë˜ìŠ¤
from transformers import Qwen2ForCausalLM

class Qwen2ForCausalLMWithLMHead(Qwen2ForCausalLM):
    def __init__(self, config):
        super().__init__(config)
        # ì´ë¯¸ lm_headê°€ ì—†ë‹¤ë©´ ìƒˆë¡œ ìƒì„±
        if not hasattr(self, "lm_head"):
            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    # forwardëŠ” ê¸°ì¡´ Qwen2ForCausalLMì´ ì•Œì•„ì„œ ì²˜ë¦¬

# === 3. config ìˆ˜ì • (architectures ë³€ê²½) ===
config = base_model.config
config.architectures = ["Qwen2ForCausalLMWithLMHead"]
# (ì›í•˜ë©´ hidden_size/vocab_sizeë„ configì— í™•ì‹¤íˆ ë„£ì–´ë‘”ë‹¤)
config.hidden_size = base_model.model.embed_tokens.embedding_dim
config.vocab_size = base_model.model.embed_tokens.num_embeddings

# === 4. ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ì— base_model state_dict ë¡œë”© ===
print("ğŸ“ Creating custom model and loading state_dict...")
model = Qwen2ForCausalLMWithLMHead(config)

# weight tying í•´ì œ + ë³µì‚¬
if hasattr(base_model, "model"):
    embed_tokens = base_model.model.embed_tokens
else:
    embed_tokens = base_model.get_input_embeddings()

# lm_head weight ë³µì‚¬(ì—°ê²° ëŠê¸°)
model.lm_head.weight = nn.Parameter(embed_tokens.weight.clone())

# ë‚˜ë¨¸ì§€ weightëŠ” base_modelì—ì„œ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ê¸°
model.load_state_dict(base_model.state_dict(), strict=False)

# === 5. í™•ì¸ ===
print("âœ… lm_head in state_dict?", 'lm_head.weight' in model.state_dict())
print("âŒ weight tied?", model.lm_head.weight.data_ptr() == embed_tokens.weight.data_ptr())

# === 6. ì €ì¥ ë° HF ì—…ë¡œë“œ ===
save_dir = "./qwen2.5-3b-with-lmhead"
print(f"ğŸ’¾ Saving model to: {save_dir}")
model.save_pretrained(save_dir, safe_serialization=True)
tokenizer.save_pretrained(save_dir)

# push to hub
print(f"â˜ï¸ Uploading to Hugging Face Hub: {HF_HUB_REPO}")
model.push_to_hub(HF_HUB_REPO, safe_serialization=True)
tokenizer.push_to_hub(HF_HUB_REPO)

print("ğŸ‰ Done! Model with lm_head uploaded.")